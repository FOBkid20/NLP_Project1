{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = json.load(open('gg2013.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_words = ['Drama', 'Musical', 'Film', 'Television', 'Motion Picture']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tweet in contents:\n",
    "    for endWord in end_words:\n",
    "        match = re.search(r'(?<=\\sBest).*(?='+ endWord +')', tweet[0], re.IGNORECASE)\n",
    "        if match:\n",
    "            award = 'Best'+ match.group(0) + endWord\n",
    "            print(award)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    tweets_data = json.load(open(filename))\n",
    "    \n",
    "    stop_words = list(stopwords.words('english')) \n",
    "    \n",
    "    gg_stop_words = ['Globe', 'RT', 'http', 'Golden', 'Globes', 'GoldenGlobes', 'Goldenglobes', 'Goldenglobe', 'gg','golden globes', 'golden globe', 'goldenglobe','goldenglobes','gg2015','gg15','goldenglobe2015','goldenglobe15','goldenglobes2015','goldenglobes15', 'gg2013','gg13','goldenglobe2013','goldenglobe13','goldenglobes2013','goldenglobes13', 'rt', '2013', '2015']\n",
    "\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    word_list = []\n",
    "    \n",
    "    for tweet in tweets_data:\n",
    "        text = tweet['text']  \n",
    "        time = tweet['timestamp_ms']\n",
    "        words = tokenizer.tokenize(text)\n",
    "        tweetText = []\n",
    "        for w in words:\n",
    "            if w not in stop_words and w not in gg_stop_words:\n",
    "                tweetText.append(w)\n",
    "        word_list.append((tweetText, time))\n",
    "        \n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = parse_file('gg2013.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tweets = []\n",
    "for t in tweets:\n",
    "    if 'host' in t[0]:\n",
    "        host_tweets.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger')\n",
    "tagged_words = []\n",
    "bigram_words = []\n",
    "for t in host_tweets:\n",
    "        bigram_words.append(list((nltk.bigrams(t[0]))))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = []\n",
    "\n",
    "for tweet in bigram_words:\n",
    "    for pair in tweet:\n",
    "        big_list.append(pair)\n",
    "big_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = []\n",
    "\n",
    "for pair in big_list:\n",
    "    tagged_words.append(nltk.pos_tag(pair))\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words_nnp = []\n",
    "\n",
    "for pair in big_list:\n",
    "    if nltk.pos_tag(pair)[0][1] == 'NNP' and nltk.pos_tag(pair)[1][1] == 'NNP':\n",
    "        tagged_words_nnp.append(pair)\n",
    "tagged_words_nnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nnp_pairs = []\n",
    "# for pair in tagged_words:\n",
    "#     if pair[0][1] == 'NNP' and pair[1][1] == 'NNP':\n",
    "#         nnp_pairs.append(pair)\n",
    "# nnp_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(tagged_words_nnp).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
